{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.contrib import text\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greg Bruss\n",
    "## KEN2570 - Spring 2019 Project\n",
    "\n",
    "In our Natural Language Processing class, we saw how we can train a word2vec word embedding model on a large scale corpus. These \"pre-trained\" word vectors can be applied in a variety of tasks. The task I will be looking at in this notebook is the task of finding analogies.\n",
    "\n",
    "An analogy is defined as a comparison between one thing and another, and represents a form of correspondence or similarity. Although the two terms compared are different, they share fundamental similarities that allows a relationship to be inferred. An example would be \"Man is to woman, as son is to daughter\". \n",
    "\n",
    "#### Why analogy finding is useful\n",
    "\"Analogical reasoning\" is an important skill that anyone who truly understands language should know. An \"analogical argument\" is an explicit representation of a form of analogical reasoning that cites accepted similarities between two systems to support the conclusion that some further similarity exists [1]. It is clear that any AI system would need to have a grasp of these similarities, particularly because much of human speech uses analogy as a valid form of expression.\n",
    "\n",
    "### Getting Pretrained Word Vectors\n",
    "\n",
    "We need a way to get the pretrained word vectors. We can make use of the GluonNLP package (https://gluon-nlp.mxnet.io/), which makes it easy to evaluate and train word embeddings, using any choice of word2vec, fastText, or GloVe models. Word2Vec Models were introduced by Mikolov et. al [3], and FastText models by Bojanowski et. al [4]. GloVe models were introduced by Pennington et al [5].\n",
    "\n",
    "We can make use of the mxnet.contrib.text API, which allows loading of pre-trained embedding vectors for text tokens and storing them in the mxnet.ndarray.NDArray format (MXNet documentation - https://mxnet.incubator.apache.org/api/python/contrib/text.html)\n",
    "\n",
    "The keys of the pretrained files will be either glove or fasttext, as these are the ones supported in the MXNet model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['glove', 'fasttext'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.embedding.get_pretrained_file_names().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Text\n",
    "\n",
    "I will use the GloVe word embedding. It is trained on the \"Wikipedia 2014 + Gigaword 5\" dataset. A quick summary of this dataset is the following:  \n",
    "    \n",
    "    6 Billion tokens  \n",
    "    400K vocab (uncased)  \n",
    "    50, 100, 200, and 300-Dimensional vectors available  \n",
    "    822 mb download  \n",
    "    \n",
    "    \n",
    "    100d, 200d, & 300d vectors are available, downloads as glove.6B.zip  \n",
    "    \n",
    "    (See https://nlp.stanford.edu/projects/glove/ for further details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove.42B.300d.txt', 'glove.6B.50d.txt', 'glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt', 'glove.840B.300d.txt', 'glove.twitter.27B.25d.txt', 'glove.twitter.27B.50d.txt', 'glove.twitter.27B.100d.txt', 'glove.twitter.27B.200d.txt']\n"
     ]
    }
   ],
   "source": [
    "print(text.embedding.get_pretrained_file_names('glove'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can instantiate a pre-trained embedding using MXNet's text.embedding.create API. In this case I will use a 300-Dimensional word embedding,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b50d = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_42b300d = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words in this pre-trained vector's dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary size of the 50d glove model is: 400001\n"
     ]
    }
   ],
   "source": [
    "print(\"The dictionary size of the 50d glove model is:\", len(glove_6b50d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary size of the 300d glove model is: 1917495\n"
     ]
    }
   ],
   "source": [
    "print(\"The dictionary size of the 300d glove model is:\", len(glove_42b300d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for this dictionary, lets look at some of the words using MXNet's index-to-token function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'knowledge' is: 2490\n",
      "Index of the word 'data' is: 934\n",
      "Index of the word 'robot' is: 9248\n",
      "Index of the word 'human' is: 474\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of the word 'knowledge' is:\", glove_6b50d.token_to_idx['knowledge'])\n",
    "print(\"Index of the word 'data' is:\", glove_6b50d.token_to_idx['data'])\n",
    "print(\"Index of the word 'robot' is:\", glove_6b50d.token_to_idx['robot'])\n",
    "print(\"Index of the word 'human' is:\", glove_6b50d.token_to_idx['human'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making use of the GloVe Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we can use the word embedding for the analogy task by searching for words that appear closer in the Vector Space, or that can be reached using a \"relationship vector\" which takes as input the analogical relationship \n",
    "\n",
    "####  \"A --> B:  C --> D\". Given A,B, relationship(A,B), and C, find D\n",
    "\n",
    "We can use something like K-Nearest Neighbours [6] for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    # The added 1e-9 is for numerical stability\n",
    "    cos = nd.dot(W, x.reshape((-1,))) / (\n",
    "        (nd.sum(W * W, axis=1) + 1e-9).sqrt() * nd.sum(x * x).sqrt())\n",
    "    topk = nd.topk(cos, k=k, ret_typ='indices').asnumpy().astype('int32')\n",
    "    return topk, [cos[i].asscalar() for i in topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the analogy problem, we need to find the word vector that is most similar to the result vector of  vec(ùëê)+vec(ùëè)‚àívec(ùëé) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    vecs = embed.get_vecs_by_tokens([token_a, token_b, token_c])\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
    "    return embed.idx_to_token[topk[0]]  # Remove unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Analogies: Will use 3 categories of analogy with 15 test cases each\n",
    "\n",
    "The different types of analogy are:\n",
    "\n",
    "#### capital: country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japan'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('beijing', 'china', 'tokyo', glove_42b300d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adjective: superlative: adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('bad', 'worst', 'big', glove_42b300d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### present-tense verb: past tense verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_analogy('accept', 'accepted', 'achieve', glove_6b_50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['after', 'before', 'ahead', 'behind'], ['anterior', 'posterior', 'backward', 'forward'], ['before', 'after', 'beginning', 'end'], ['below', 'above', 'climb', 'descend'], ['dead', 'alive', 'decrement', 'increment'], ['descend', 'ascend', 'dive', 'emerge'], ['down', 'up', 'downslope', 'upslope'], ['drop', 'lift', 'dynamic', 'static'], ['employ', 'dismiss', 'exit', 'entrance'], ['fall', 'rise', 'first', 'last']]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"antonyms.txt\", \"r\")\n",
    "analogies = f.readlines()\n",
    "for i in range(len(analogies)):\n",
    "    analogies[i] = analogies[i].lower()\n",
    "    analogies[i]=analogies[i].split()\n",
    "\n",
    "print(analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_answer = []\n",
    "for i in range(len(analogies)):\n",
    "    real_answer.append(analogies[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answer=[get_analogy(analogy[0], analogy[1], analogy[2], glove_6b50d) for analogy in analogies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ahead', 'backward', 'beginning', 'climb', 'decrement', 'dive', 'downslope', 'dynamic', 'exit', 'first']\n",
      "['behind', 'forward', 'end', 'descend', 'increment', 'emerge', 'upslope', 'static', 'entrance', 'last']\n"
     ]
    }
   ],
   "source": [
    "print(predicted_answer)\n",
    "print(real_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(predicted_answer)\n",
    "correct = 0\n",
    "for i in range(len(predicted_answer)):\n",
    "    if predicted_answer[i] == real_answer[i]:\n",
    "                correct +=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct:  0\n",
      "Total asked:  10\n",
      "The accuracy on the  antonyms.txt dataset is:  0.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct/total * 100\n",
    "print(\"Total correct: \", correct)\n",
    "print(\"Total asked: \", total)\n",
    "print(\"The accuracy on the \", f.name,\"dataset is: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1] Bartha, Paul, \"Analogy and Analogical Reasoning\", The Stanford Encyclopedia of Philosophy (Spring 2019 Edition), Edward N. Zalta (ed.), URL = <https://plato.stanford.edu/archives/spr2019/entries/reasoning-analogy/>.\n",
    "\n",
    "[2] Zhang, A, \"Dive into Deep Learning\" (2019), Z. Lipton, M. Li, A. Smola URL = https://d2l.ai/\n",
    "\n",
    "[3] Mikovol et al, ‚ÄúEfficient estimation of word representations in vector space‚Äù ICLR Workshop 2013.\n",
    "\n",
    "[4] Bojanowski et al., ‚ÄúEnriching word vectors with subword information‚Äù TACL 2017.\n",
    "\n",
    "[5] Pennington et al., ‚ÄúGlove: global vectors for word representation‚Äù, ACL 2014.\n",
    "\n",
    "[6] https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
